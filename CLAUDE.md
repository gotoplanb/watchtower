# CLAUDE.md — Watchtower

Context for Claude Code sessions working on this project.

## What This Project Is

A local Grafana LGTM observability stack on a kind cluster. Grafana Alloy is the OTLP telemetry router that dual-writes to both local backends (Tempo, Loki, Mimir) and Sumo Logic. The spec lives at `watchtower-spec.pdf`.

## Host Ports

Standard ports are remapped to avoid conflicts with other local services (lxp-e2e uses 3000/4317/4318):

| Service | Host Port | In-Cluster Port | kind NodePort |
|---------|-----------|-----------------|---------------|
| Grafana | 13000 | 80 | 30000 |
| OTLP gRPC | 14317 | 4317 | 30317 |
| OTLP HTTP | 14318 | 4318 | 30318 |

If you ever change ports, update: `kind/cluster-config.yaml`, `scripts/port-forward.sh`, `Makefile`, and `README.md`.

## Helm Charts and Versions

All charts are from the `grafana` Helm repo (`https://grafana.github.io/helm-charts`). Values live in `helm/values/`. The charts used:

| Component | Chart | Notes |
|-----------|-------|-------|
| Tempo | `grafana/tempo` | Simple single-binary, local storage |
| Loki | `grafana/loki` | SingleBinary deployment mode |
| Mimir | `grafana/mimir-distributed` | Distributed mode, most complex — see gotchas below |
| Grafana | `grafana/grafana` | Pre-provisioned datasources with cross-references |
| Alloy | `grafana/alloy` | External ConfigMap, not chart-managed config |

## Mimir Chart Gotchas (mimir-distributed ~3.0.1)

This chart has aggressive defaults that assume a production cloud setup. For local dev with filesystem storage, all of these are required:

1. **Kafka/ingest_storage**: Chart defaults to `ingest_storage.enabled: true` with Kafka. You MUST set both `mimir.structuredConfig.ingest_storage.enabled: false` AND `kafka.enabled: false`.
2. **push_grpc_method_enabled**: When ingest_storage is disabled, the chart still sets `push_grpc_method_enabled: false` (a Kafka-only setting). You MUST set `ingester.push_grpc_method_enabled: true` in structuredConfig or the ingester will refuse writes.
3. **multitenancy_enabled**: Must be `false` in structuredConfig, otherwise all writes from Alloy get 401 "no org id" because we don't send X-Scope-OrgID headers.
4. **query_scheduler**: Do NOT disable it. The query-frontend hard-depends on query_scheduler and will CrashLoop with "missing address" if it's gone.
5. **Compactor path overlap**: `blocks_storage.filesystem.dir` must not be under `/data` or it overlaps with compactor's data_dir. Current setup uses `/data/mimir-blocks` for blocks and `/data/mimir-compactor` for compactor.
6. **zoneAwareReplication**: Must be `false` on ingester and store_gateway for single-replica local dev.
7. **rollout_operator**: Disable it — not needed for local dev and adds an extra pod.

## Loki Chart Gotchas

- Disable `chunksCache` and `resultsCache` — they deploy memcached pods that won't schedule on a resource-constrained kind cluster.
- `auth_enabled: false` is required since Alloy doesn't send tenant headers.
- Use `deploymentMode: SingleBinary` and set `backend/read/write` replicas to 0.

## Alloy Component Names (v1.12.2)

The spec's component names may be outdated. Verified working components:

- `otelcol.exporter.prometheusremotewrite` does **NOT exist**. Use `otelcol.exporter.prometheus` + `prometheus.remote_write` as a two-stage pipeline instead.
- For Tempo traces, use `otelcol.exporter.otlp` (gRPC on port 4317), NOT `otelcol.exporter.otlphttp`. Tempo's gRPC receiver is more reliable.
- The `otelcol.exporter.loki` + `loki.write` pipeline works correctly for logs.

## Alloy Config Management

Alloy's config is managed as a standalone file, not generated by the Helm chart:

- `alloy/config.alloy` — dual-write (local LGTM + Sumo Logic)
- `alloy/config-local-only.alloy` — local LGTM only, no Sumo dependency

The config is loaded into a ConfigMap (`alloy-config`) during `make deploy-alloy`. The Helm chart is told `configMap.create: false` so it uses our external ConfigMap.

Switching between configs: `make enable-local-only` / `make disable-local-only`.

## Alloy NodePort Patch

The Alloy Helm chart's `extraPorts` adds ports to the service, but does NOT let you set specific `nodePort` values. After `make deploy-alloy`, the OTLP ports get random NodePorts. A `kubectl patch` is needed to pin them to 30317/30318 so the kind port mappings work:

```bash
kubectl patch svc alloy -n watchtower --type='json' -p='[
  {"op":"replace","path":"/spec/ports/1/nodePort","value":30317},
  {"op":"replace","path":"/spec/ports/2/nodePort","value":30318}
]'
```

This patch does NOT survive `helm upgrade`. If you redeploy Alloy, re-run the patch. This is a known limitation — a future fix could use a post-deploy hook or a custom service manifest.

## Sumo Logic

- OTLP endpoint is stored in K8s secret `sumo-credentials` (key: `endpoint`) in the `watchtower` namespace.
- Alloy reads it via `env("SUMO_OTLP_ENDPOINT")` using `extraEnv` with `secretKeyRef` in `helm/values/alloy.yaml`.
- If the secret is missing, Alloy will CrashLoop. Switch to local-only mode if Sumo isn't needed.

## Grafana Datasources

Pre-provisioned in `helm/values/grafana.yaml` with cross-references:

- **Mimir** (prometheus type) — default datasource, at `mimir-query-frontend:8080/prometheus`
- **Tempo** — links to Loki (traces-to-logs) and Mimir (service map, traces-to-metrics)
- **Loki** — derived field regex extracts trace_id and links to Tempo

Datasource UIDs are auto-generated by Grafana on first deploy. If you need to reference them in dashboards, query them from the Grafana API: `curl http://localhost:13000/api/datasources`.

## Test Data Generator

`test-data/generate.py` uses the OpenTelemetry Python SDK to send synthetic traces, metrics, and logs via OTLP gRPC. Uses a venv at `test-data/.venv`.

```bash
cd test-data && source .venv/bin/activate
python generate.py --endpoint localhost:14317 --rate 10 --error-rate 0.05
```

The `make test-data` target in the Makefile still uses system pip — prefer using the venv directly as shown above.

## Common Operations

```bash
make deploy-alloy       # Redeploy Alloy after config changes (remember to re-patch NodePorts)
make render             # Dump rendered Helm templates to helm/rendered/ for learning
make logs               # Tail Alloy logs for debugging
make status             # Quick pod + service health check
```

## Things That Could Be Improved

- Make the Alloy NodePort patch durable (post-install hook or separate service manifest)
- Update `make test-data` to use the venv instead of system pip
- Build Grafana dashboards in `dashboards/` (currently empty)
- OTLP histogram/counter metrics from the generator may appear sparse in Mimir — the `otelcol.exporter.prometheus` conversion and flush interval may need tuning
